{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "USI 19/20 Deep Learning 2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhDroid-Fin/USI-deep-learning-lab-19-20/blob/master/USI_19_20_Deep_Learning_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QSxo0oCKkeY",
        "colab_type": "text"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tryEoSy0Kp0e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist \n",
        "from tensorflow.keras import utils\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "om4JlDJtXy7j",
        "colab_type": "text"
      },
      "source": [
        "# Activity 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJZuTNoaYFXZ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Exercise 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90haBGo0K4GS",
        "colab_type": "text"
      },
      "source": [
        "### Exercise questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUI39UpRKzv-",
        "colab_type": "text"
      },
      "source": [
        "Using the code in Slides 46-49 as a starting point, try to find **a better set of hyperparameters** to train a multilayer perceptron to perform MNIST digit classification. You may try different \n",
        "\n",
        "1. learning rates, \n",
        "2. batch sizes, \n",
        "3. number of epochs, \n",
        "4. number of neurons per hidden layer, \n",
        "5. number of hidden layers (requires more significant changes), \n",
        "6. activation functions, and \n",
        "7. optimization methods. \n",
        "\n",
        "Remember to base your decisions exclusively on validation set loss and/or accuracy. Once you settle on a final model, compute the test set accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA4BmRqr8uKc",
        "colab_type": "text"
      },
      "source": [
        "### Intro of MNIST datast:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRNk9siXK_VA",
        "colab_type": "text"
      },
      "source": [
        "  0. It contains handwritten numbers from 0 to 9\n",
        "  1. The shape of X in the training set is (60000, 28, 28)\n",
        "  2. The shape of y in the training set is (60000, )\n",
        "  3. The shape of X in the test set is (10000, 28, 28)\n",
        "  4. The shape of y in the test set is (10000, )\n",
        "In `batch_iterator`, each observation is reshaped from 2D into 1D and adjusted based on some color scale."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cZH_S9Oyzmf",
        "colab_type": "code",
        "outputId": "7e3896e0-fa04-4277-c999-4a2498be340a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        }
      },
      "source": [
        "# Understand the dataset\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist \n",
        "from tensorflow.keras import utils\n",
        "(X, y), (X_test, y_test) = mnist.load_data() \n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "# Understand the data points\n",
        "import matplotlib.pyplot as plt\n",
        "first_array=X[0,]/255.\n",
        "plt.imshow(first_array)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSRB2IAnCDiTx/65XcTNOWsh5AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQbLN6h_D8Iz",
        "colab_type": "text"
      },
      "source": [
        "### Why create `batch_iterator` ?: subsampling\n",
        "This is basically an efficient way to read elements from the Dataset in some order.\n",
        "Every time the iterator extract one instance from the MNIST dataset, non-repeatedly.\n",
        "In this way, it can save memory for storing all the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PK_sN1UvkSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist \n",
        "from tensorflow.keras import utils\n",
        "\n",
        "# check the link for understanding the codes: https://zhuanlan.zhihu.com/p/30751039\n",
        "# sdg uses subsamples to run gradient descent \n",
        "def batch_iterator(X, y, batch_size): \n",
        "  X = X.reshape(X.shape[0], 784)/255.           # reshape (28,28) into (784,)\n",
        "  y = utils.to_categorical(y, num_classes=10)   # convert targets into categorical numbers (like Ind = 1 if yes, Ind = 0 otw)\n",
        "\n",
        "  data = tf.data.Dataset.from_tensor_slices((X, y)) # assign the data to tensors\n",
        "  data = data.shuffle(buffer_size=X.shape[0])       # shuffle the data; where buffer size is the number of elements from this dataset from which the new dataset will sample.\n",
        "  data = data.repeat()                              # repeat的功能就是将整个序列重复多次，使用repeat(5)就可以将之变成5个epoch; 如果直接调用repeat()的话，生成的序列就会无限重复下去\n",
        "  data = data.batch(batch_size=batch_size)          # batch就是将多个元素组合成batch，batch_size = 32 将dataset中的每个元素组成了大小为32的batch：\n",
        "\n",
        "  return data.make_one_shot_iterator().get_next()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d36m4k03XQfb",
        "colab_type": "text"
      },
      "source": [
        "### Deep learning for image recognition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A90_o0ndD40d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(): \n",
        "  tf.reset_default_graph() \n",
        "  tf.set_random_seed(seed=0)\n",
        "\n",
        "  # Loads and splits MNIST dataset\n",
        "  train_size = 55000\n",
        "  batch_size = 64\n",
        "  (X_trainval, y_trainval), (X_test, y_test) = mnist.load_data() \n",
        "  X_train, y_train = X_trainval[:train_size], y_trainval[:train_size] \n",
        "  X_val, y_val = X_trainval[train_size:], y_trainval[train_size:]\n",
        "  train_iter = batch_iterator(X_train, y_train, batch_size)\n",
        "\n",
        "  # Note: You may want to use smaller batches on a GPU\n",
        "  val_iter = batch_iterator(X_val, y_val, X_val.shape[0])\n",
        "  test_iter = batch_iterator(X_test, y_test, X_val.shape[0]) # Subsampling\n",
        "\n",
        "  # Training procedure hyperparameters\n",
        "  learning_rate = 1e-3 \n",
        "  n_epochs = 16 \n",
        "  verbose_freq = 2000\n",
        "\n",
        "  # Model hyperparameters\n",
        "  n_neurons_1 = 784 # Number of input neurons (28 x 28 x 1)\n",
        "  n_neurons_2 = 100 # Number of neurons in the second layer (first hidden) \n",
        "  n_neurons_3 = 100 # Number of neurons in the third layer (second hidden) \n",
        "  n_neurons_4 = 10 # Number of output neurons (and classes)\n",
        "  X = tf.placeholder(tf.float32, [None, n_neurons_1]) \n",
        "  Y = tf.placeholder(tf.float32, [None, n_neurons_4])\n",
        "\n",
        "  # Model parameters. Important: should not be initialized to zero\n",
        "  W2 = tf.Variable(tf.truncated_normal([n_neurons_1, n_neurons_2])) \n",
        "  W3 = tf.Variable(tf.truncated_normal([n_neurons_2, n_neurons_3])) \n",
        "  W4 = tf.Variable(tf.truncated_normal([n_neurons_3, n_neurons_4]))\n",
        "  b2 = tf.Variable(tf.zeros(n_neurons_2)) \n",
        "  b3 = tf.Variable(tf.zeros(n_neurons_3)) \n",
        "  b4 = tf.Variable(tf.zeros(n_neurons_4))\n",
        "  \n",
        "  # Model definition\n",
        "  # The rectified linear activation function is given by a = max(z, 0) \n",
        "  A2 = tf.nn.relu(tf.matmul(X, W2) + b2)                                        # Hyperparameter: activation functions\n",
        "  A3 = tf.nn.relu(tf.matmul(A2, W3) + b3)\n",
        "  Z4 = tf.matmul(A3, W4) + b4\n",
        "\n",
        "  # Loss definition\n",
        "  # Important: this function expects weighted inputs, not activations \n",
        "  loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=Z4) \n",
        "  loss = tf.reduce_mean(loss)\n",
        "  hits = tf.equal(tf.argmax(Z4, axis=1), tf.argmax(Y, axis=1)) \n",
        "  accuracy = tf.reduce_mean(tf.cast(hits, tf.float32))\n",
        "\n",
        "  # Using Adam instead of gradient descent\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate)                             # Hyperparameter: optimizer \n",
        "  train = optimizer.minimize(loss)\n",
        "\n",
        "  # Allows saving model to disc\n",
        "  saver = tf.train.Saver() \n",
        "  session = tf.Session()\n",
        "  session.run(tf.global_variables_initializer())\n",
        "\n",
        "  # Using mini-batches instead of entire dataset\n",
        "  n_batches = n_epochs * (train_size // batch_size) # roughly \n",
        "  for t in range(n_batches):\n",
        "    X_batch, Y_batch = session.run(train_iter) \n",
        "    session.run(train, {X: X_batch, Y: Y_batch})\n",
        "\n",
        "    # Computes validation loss every `verbose_freq` batches\n",
        "    if verbose_freq > 0 and t % verbose_freq == 0: \n",
        "      X_batch, Y_batch = session.run(val_iter)\n",
        "      l = session.run(loss, {X: X_batch, Y: Y_batch})\n",
        "      print('Batch: {0}. Validation loss: {1}.'.format(t, l))\n",
        "\n",
        "  saver.save(session, '/tmp/mnist.ckpt') \n",
        "  session.close()\n",
        "\n",
        "  # Loading model from file\n",
        "  session = tf.Session() \n",
        "  saver.restore(session, '/tmp/mnist.ckpt')\n",
        "\n",
        "  # In a proper experiment, test set results are computed only once, and \n",
        "  # absolutely never considered during the choice of hyperparameters X_batch, Y_batch = session.run(test_iter)\n",
        "  acc = session.run(accuracy, {X: X_batch, Y: Y_batch})\n",
        "  print('Test accuracy: {0}.'.format(acc)) \n",
        "\n",
        "  session.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrZSi_MzXKls",
        "colab_type": "code",
        "outputId": "eb1d1080-4897-4363-c733-ae2fc4338ec0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch: 0. Validation loss: 434.2591857910156.\n",
            "Batch: 2000. Validation loss: 5.4443440437316895.\n",
            "Batch: 4000. Validation loss: 3.2992324829101562.\n",
            "Batch: 6000. Validation loss: 2.3997390270233154.\n",
            "Batch: 8000. Validation loss: 1.9716074466705322.\n",
            "Batch: 10000. Validation loss: 1.8439974784851074.\n",
            "Batch: 12000. Validation loss: 1.690049648284912.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/mnist.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /tmp/mnist.ckpt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.953125.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJP3pGy-XVjq",
        "colab_type": "text"
      },
      "source": [
        "### Tuning the hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWbffo8aIPhj",
        "colab_type": "text"
      },
      "source": [
        "#### Create a new function of hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt6GPRXzWawB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def exercise1(learning_rate,batch_size,n_epochs,layer_structure): \n",
        "    tf.reset_default_graph() \n",
        "    tf.set_random_seed(seed=0)\n",
        "\n",
        "    # Loads and splits MNIST dataset\n",
        "    train_size = 55000\n",
        "    (X_trainval, y_trainval), (X_test, y_test) = mnist.load_data() \n",
        "    X_train, y_train = X_trainval[:train_size], y_trainval[:train_size] \n",
        "    X_val, y_val = X_trainval[train_size:], y_trainval[train_size:]\n",
        "    train_iter = batch_iterator(X_train, y_train, batch_size)\n",
        "\n",
        "    # Note: You may want to use smaller batches on a GPU\n",
        "    val_iter = batch_iterator(X_val, y_val, X_val.shape[0])\n",
        "    test_iter = batch_iterator(X_test, y_test, X_val.shape[0]) # Subsampling\n",
        "\n",
        "    # Training procedure hyperparameters\n",
        "    verbose_freq = 2000\n",
        "\n",
        "    # Model hyperparameters\n",
        "    X = tf.placeholder(tf.float32, [None, layer_structure[0]]) \n",
        "    Y = tf.placeholder(tf.float32, [None, layer_structure[-1]])\n",
        "\n",
        "    # Model parameters. Important: should not be initialized to zero\n",
        "    W = []\n",
        "    b = []\n",
        "    for col in range(layer_structure.shape[0]-1):\n",
        "      W.append(tf.Variable(tf.truncated_normal([layer_structure[col], layer_structure[col+1]]))) \n",
        "      b.append(tf.Variable(tf.zeros(layer_structure[col+1])))  \n",
        "\n",
        "    # Model definition\n",
        "    # The rectified linear activation function is given by a = max(z, 0) \n",
        "    ActFcn = []\n",
        "    ActFcn.append(tf.nn.relu(tf.matmul(X, W[0]) + b[0]))                         # Hyperparameter: activation functions\n",
        "    \n",
        "    for col in range(1,layer_structure.shape[0]-2):\n",
        "      ActFcn.append(tf.nn.relu(tf.matmul(ActFcn[col-1], W[col]) + b[col]))\n",
        "\n",
        "    Output = tf.matmul(ActFcn[-1], W[-1]) + b[-1]\n",
        "\n",
        "    # Loss definition\n",
        "    # Important: this function expects weighted inputs, not activations \n",
        "    loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=Output) \n",
        "    loss = tf.reduce_mean(loss)\n",
        "    hits = tf.equal(tf.argmax(Output, axis=1), tf.argmax(Y, axis=1)) \n",
        "    accuracy = tf.reduce_mean(tf.cast(hits, tf.float32))\n",
        "\n",
        "    # Using Adam instead of gradient descent\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate)                            # Hyperparameter: optimizer \n",
        "    train = optimizer.minimize(loss)\n",
        "\n",
        "    # Allows saving model to disc\n",
        "    saver = tf.train.Saver() \n",
        "    session = tf.Session()\n",
        "    session.run(tf.global_variables_initializer())\n",
        "\n",
        "    # Using mini-batches instead of entire dataset\n",
        "    n_batches = n_epochs * (train_size // batch_size) # roughly \n",
        "    for t in range(n_batches):\n",
        "      X_batch, Y_batch = session.run(train_iter) \n",
        "      session.run(train, {X: X_batch, Y: Y_batch})\n",
        "\n",
        "      # Computes validation loss every `verbose_freq` batches\n",
        "      if verbose_freq > 0 and t % verbose_freq == 0: \n",
        "        X_batch, Y_batch = session.run(val_iter)\n",
        "        l = session.run(loss, {X: X_batch, Y: Y_batch})\n",
        "        print('Batch: {0}. Validation loss: {1}.'.format(t, l))\n",
        "\n",
        "    saver.save(session, '/tmp/mnist.ckpt') \n",
        "    session.close()\n",
        "\n",
        "    # Loading model from file\n",
        "    session = tf.Session() \n",
        "    saver.restore(session, '/tmp/mnist.ckpt')\n",
        "\n",
        "    # In a proper experiment, test set results are computed only once, and \n",
        "    # absolutely never considered during the choice of hyperparameters X_batch, Y_batch = session.run(test_iter)\n",
        "    acc = session.run(accuracy, {X: X_batch, Y: Y_batch})\n",
        "    print('Test accuracy: {0}.'.format(acc)) \n",
        "\n",
        "    session.close()\n",
        "    return (acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi8gfrc06dGM",
        "colab_type": "code",
        "outputId": "730007c2-e139-44e4-ca58-f9e6a71cc720",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "learning_rate = 1e-3 \n",
        "batch_size = 64\n",
        "n_epochs = 16 \n",
        "layer_structure = np.array([784, 100, 100, 10])\n",
        "\n",
        "exercise1(learning_rate,batch_size,n_epochs,layer_structure)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch: 0. Validation loss: 272.5928955078125.\n",
            "Batch: 2000. Validation loss: 5.680078029632568.\n",
            "Batch: 4000. Validation loss: 3.3058738708496094.\n",
            "Batch: 6000. Validation loss: 2.4212734699249268.\n",
            "Batch: 8000. Validation loss: 1.9688562154769897.\n",
            "Batch: 10000. Validation loss: 1.7496371269226074.\n",
            "Batch: 12000. Validation loss: 1.5828067064285278.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/mnist.ckpt\n",
            "Test accuracy: 0.953125.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RA56GDfOHvgO",
        "colab_type": "text"
      },
      "source": [
        "The test accuracy is exactly the same as `main()`. This shows that the new code actually works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTBz5m3RIXO9",
        "colab_type": "text"
      },
      "source": [
        "### Change hyperparameters to achieve the highest accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6tS06-AIc_7",
        "colab_type": "text"
      },
      "source": [
        "##### Learning rates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tb0eCacxa9rm",
        "colab_type": "text"
      },
      "source": [
        "Create Pandas Dataframe: create dictionary first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlBBNg7RYkUo",
        "colab_type": "code",
        "outputId": "084a3bb0-f3f8-4777-c603-4d15c1cbe6c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# importing pandas \n",
        "import pandas as pd \n",
        "  \n",
        "# Creating new dataframe \n",
        "initial_data = {'First_name': ['Ram', 'Mohan', 'Tina', 'Jeetu', 'Meera'],  \n",
        "                'Last_name': ['Kumar', 'Sharma', 'Ali', 'Gandhi', 'Kumari'],  \n",
        "                'Marks': [12, 52, 36, 85, 23] } \n",
        "  \n",
        "df = pd.DataFrame(initial_data, columns = ['First_name', 'Last_name', 'Marks']) \n",
        "  \n",
        "# Generate result using pandas \n",
        "result = [] \n",
        "for value in df[\"Marks\"]: \n",
        "    if value >= 33: \n",
        "        result.append(\"Pass\") \n",
        "    elif value < 0 and value > 100: \n",
        "        result.append(\"Invalid\") \n",
        "    else: \n",
        "        result.append(\"Fail\") \n",
        "       \n",
        "df[\"Result\"] = result    \n",
        "print(df) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  First_name Last_name  Marks Result\n",
            "0        Ram     Kumar     12   Fail\n",
            "1      Mohan    Sharma     52   Pass\n",
            "2       Tina       Ali     36   Pass\n",
            "3      Jeetu    Gandhi     85   Pass\n",
            "4      Meera    Kumari     23   Fail\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9_6ScY0IgBY",
        "colab_type": "code",
        "outputId": "2f6edb3f-d080-46b2-b508-62f22e4c78db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        }
      },
      "source": [
        "range_lr = np.array([1e-5, 1e-4, 1e-3, 1e-2, 1e-1])\n",
        "batch_size = 64\n",
        "n_epochs = 16 \n",
        "layer_structure = np.array([784, 100, 100, 10])\n",
        "\n",
        "acc = []\n",
        "for learning_rate in range_lr:\n",
        "  print('---------- learning rate = {0} ----------'.format(learning_rate))\n",
        "  acc.append(exercise1(learning_rate,batch_size,n_epochs,layer_structure))\n",
        "\n",
        "df = pd.DataFrame(data = None, index = None)\n",
        "df['learning rate'] = range_lr\n",
        "df['test accuracy'] = acc\n",
        "print(df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------- learning rate = 1e-05 ----------\n",
            "Batch: 0. Validation loss: 279.1715087890625.\n",
            "Batch: 2000. Validation loss: 182.6839141845703.\n",
            "Batch: 4000. Validation loss: 126.70819854736328.\n",
            "Batch: 6000. Validation loss: 94.10690307617188.\n",
            "Batch: 8000. Validation loss: 73.3630599975586.\n",
            "Batch: 10000. Validation loss: 59.33284378051758.\n",
            "Batch: 12000. Validation loss: 49.64668273925781.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/mnist.ckpt\n",
            "Test accuracy: 0.515625.\n",
            "---------- learning rate = 0.0001 ----------\n",
            "Batch: 0. Validation loss: 278.559326171875.\n",
            "Batch: 2000. Validation loss: 34.69496154785156.\n",
            "Batch: 4000. Validation loss: 18.53762435913086.\n",
            "Batch: 6000. Validation loss: 12.718649864196777.\n",
            "Batch: 8000. Validation loss: 9.830509185791016.\n",
            "Batch: 10000. Validation loss: 8.197586059570312.\n",
            "Batch: 12000. Validation loss: 7.068456172943115.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/mnist.ckpt\n",
            "Test accuracy: 0.84375.\n",
            "---------- learning rate = 0.001 ----------\n",
            "Batch: 0. Validation loss: 272.5929260253906.\n",
            "Batch: 2000. Validation loss: 5.677412509918213.\n",
            "Batch: 4000. Validation loss: 3.3106770515441895.\n",
            "Batch: 6000. Validation loss: 2.452937126159668.\n",
            "Batch: 8000. Validation loss: 2.0035061836242676.\n",
            "Batch: 10000. Validation loss: 1.8659588098526.\n",
            "Batch: 12000. Validation loss: 1.612976312637329.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/mnist.ckpt\n",
            "Test accuracy: 0.96875.\n",
            "---------- learning rate = 0.01 ----------\n",
            "Batch: 0. Validation loss: 227.38197326660156.\n",
            "Batch: 2000. Validation loss: 0.6219130158424377.\n",
            "Batch: 4000. Validation loss: 0.32620763778686523.\n",
            "Batch: 6000. Validation loss: 0.24962538480758667.\n",
            "Batch: 8000. Validation loss: 0.19979119300842285.\n",
            "Batch: 10000. Validation loss: 0.15369780361652374.\n",
            "Batch: 12000. Validation loss: 0.15018592774868011.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/mnist.ckpt\n",
            "Test accuracy: 0.96875.\n",
            "---------- learning rate = 0.1 ----------\n",
            "Batch: 0. Validation loss: 356.65936279296875.\n",
            "Batch: 2000. Validation loss: 0.3695966303348541.\n",
            "Batch: 4000. Validation loss: 0.7041354775428772.\n",
            "Batch: 6000. Validation loss: 0.908984363079071.\n",
            "Batch: 8000. Validation loss: 1.4985498189926147.\n",
            "Batch: 10000. Validation loss: 1.8764863014221191.\n",
            "Batch: 12000. Validation loss: 1.8803765773773193.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/mnist.ckpt\n",
            "Test accuracy: 0.15625.\n",
            "   learning rate  test accuracy\n",
            "0        0.00001       0.515625\n",
            "1        0.00010       0.843750\n",
            "2        0.00100       0.968750\n",
            "3        0.01000       0.968750\n",
            "4        0.10000       0.156250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDAyFBOWcL0q",
        "colab_type": "code",
        "outputId": "b50cd81b-a9a9-45ea-86c5-ab5225c3d692",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "range_lr = np.arange(2*1e-3,9*1e-3,1e-3)\n",
        "print(range_lr)\n",
        "\n",
        "batch_size = 64\n",
        "n_epochs = 16 \n",
        "layer_structure = np.array([784, 100, 100, 10])\n",
        "\n",
        "acc = []\n",
        "for learning_rate in range_lr:\n",
        "  print('---------- learning rate = {0} ----------'.format(learning_rate))\n",
        "  acc.append(exercise1(learning_rate,batch_size,n_epochs,layer_structure))\n",
        "\n",
        "df = pd.DataFrame(data = None, index = None)\n",
        "df['learning rate'] = map(str,range_lr)\n",
        "df['test accuracy'] = acc\n",
        "print(df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.002 0.003 0.004 0.005 0.006 0.007 0.008 0.009]\n",
            "---------- learning rate = 0.002 ----------\n",
            "Batch: 0. Validation loss: 266.2911071777344.\n",
            "Batch: 2000. Validation loss: 3.2243237495422363.\n",
            "Batch: 4000. Validation loss: 1.9050565958023071.\n",
            "Batch: 6000. Validation loss: 1.385681390762329.\n",
            "Batch: 8000. Validation loss: 1.2957308292388916.\n",
            "Batch: 10000. Validation loss: 1.1056764125823975.\n",
            "Batch: 12000. Validation loss: 0.9556248188018799.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/mnist.ckpt\n",
            "Test accuracy: 0.984375.\n",
            "---------- learning rate = 0.003 ----------\n",
            "Batch: 0. Validation loss: 260.3132629394531.\n",
            "Batch: 2000. Validation loss: 2.2619500160217285.\n",
            "Batch: 4000. Validation loss: 1.5417578220367432.\n",
            "Batch: 6000. Validation loss: 1.0682423114776611.\n",
            "Batch: 8000. Validation loss: 0.841545581817627.\n",
            "Batch: 10000. Validation loss: 0.8402359485626221.\n",
            "Batch: 12000. Validation loss: 0.7123844623565674.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/mnist.ckpt\n",
            "Test accuracy: 0.984375.\n",
            "---------- learning rate = 0.004 ----------\n",
            "Batch: 0. Validation loss: 254.68820190429688.\n",
            "Batch: 2000. Validation loss: 1.8131953477859497.\n",
            "Batch: 4000. Validation loss: 1.1385831832885742.\n",
            "Batch: 6000. Validation loss: 0.8971213698387146.\n",
            "Batch: 8000. Validation loss: 0.7439353466033936.\n",
            "Batch: 10000. Validation loss: 0.5329253077507019.\n",
            "Batch: 12000. Validation loss: 0.5266575217247009.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/mnist.ckpt\n",
            "Test accuracy: 0.984375.\n",
            "---------- learning rate = 0.005 ----------\n",
            "Batch: 0. Validation loss: 249.36997985839844.\n",
            "Batch: 2000. Validation loss: 1.4082967042922974.\n",
            "Batch: 4000. Validation loss: 0.8415318131446838.\n",
            "Batch: 6000. Validation loss: 0.6291553974151611.\n",
            "Batch: 8000. Validation loss: 0.4866698384284973.\n",
            "Batch: 10000. Validation loss: 0.423688679933548.\n",
            "Batch: 12000. Validation loss: 0.3545159101486206.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/mnist.ckpt\n",
            "Test accuracy: 0.984375.\n",
            "---------- learning rate = 0.006 ----------\n",
            "Batch: 0. Validation loss: 244.36685180664062.\n",
            "Batch: 2000. Validation loss: 1.2182214260101318.\n",
            "Batch: 4000. Validation loss: 0.7624403238296509.\n",
            "Batch: 6000. Validation loss: 0.49815264344215393.\n",
            "Batch: 8000. Validation loss: 0.3293451964855194.\n",
            "Batch: 10000. Validation loss: 0.37476563453674316.\n",
            "Batch: 12000. Validation loss: 0.20313869416713715.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/mnist.ckpt\n",
            "Test accuracy: 0.96875.\n",
            "---------- learning rate = 0.007 ----------\n",
            "Batch: 0. Validation loss: 239.66119384765625.\n",
            "Batch: 2000. Validation loss: 1.1506977081298828.\n",
            "Batch: 4000. Validation loss: 0.5912560224533081.\n",
            "Batch: 6000. Validation loss: 0.37440091371536255.\n",
            "Batch: 8000. Validation loss: 0.2560740113258362.\n",
            "Batch: 10000. Validation loss: 0.21678857505321503.\n",
            "Batch: 12000. Validation loss: 0.18195731937885284.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/mnist.ckpt\n",
            "Test accuracy: 0.96875.\n",
            "---------- learning rate = 0.008 ----------\n",
            "Batch: 0. Validation loss: 235.2909698486328.\n",
            "Batch: 2000. Validation loss: 0.9566376805305481.\n",
            "Batch: 4000. Validation loss: 0.4629991352558136.\n",
            "Batch: 6000. Validation loss: 0.26026904582977295.\n",
            "Batch: 8000. Validation loss: 0.2510909140110016.\n",
            "Batch: 10000. Validation loss: 0.18886028230190277.\n",
            "Batch: 12000. Validation loss: 0.1531713902950287.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/mnist.ckpt\n",
            "Test accuracy: 0.984375.\n",
            "---------- learning rate = 0.009000000000000001 ----------\n",
            "Batch: 0. Validation loss: 231.21205139160156.\n",
            "Batch: 2000. Validation loss: 0.7858632802963257.\n",
            "Batch: 4000. Validation loss: 0.43524986505508423.\n",
            "Batch: 6000. Validation loss: 0.24939130246639252.\n",
            "Batch: 8000. Validation loss: 0.19852572679519653.\n",
            "Batch: 10000. Validation loss: 0.18529953062534332.\n",
            "Batch: 12000. Validation loss: 0.18663911521434784.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/mnist.ckpt\n",
            "Test accuracy: 1.0.\n",
            "   learning rate  test accuracy\n",
            "0          0.002       0.984375\n",
            "1          0.003       0.984375\n",
            "2          0.004       0.984375\n",
            "3          0.005       0.984375\n",
            "4          0.006       0.968750\n",
            "5          0.007       0.968750\n",
            "6          0.008       0.984375\n",
            "7          0.009       1.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDIWvFSehipC",
        "colab_type": "text"
      },
      "source": [
        "**Best learning rate = 0.009**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZbAEUMbi6sY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrZ0dcxNLbpi",
        "colab_type": "text"
      },
      "source": [
        "##### Batch size\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7XlDHFhLjsl",
        "colab_type": "code",
        "outputId": "5c375757-30ff-4738-80b9-aa423d16436d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "best_learning_rate = 0.009\n",
        "\n",
        "range_bs = np.array([16, 32, 64, 128])\n",
        "print(range_bs)\n",
        "\n",
        "n_epochs = 16 \n",
        "layer_structure = np.array([784, 100, 100, 10])\n",
        "\n",
        "acc = []\n",
        "for batch_size in range_bs:\n",
        "  print('---------- batch size = {0} ----------'.format(batch_size))\n",
        "  acc.append(exercise1(best_learning_rate,batch_size,n_epochs,layer_structure))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 16  32  64 128]\n",
            "---------- batch size = 16 ----------\n",
            "Batch: 0. Validation loss: 258.5850524902344.\n",
            "Batch: 2000. Validation loss: 1.7008551359176636.\n",
            "Batch: 4000. Validation loss: 0.5796899199485779.\n",
            "Batch: 6000. Validation loss: 0.2975771129131317.\n",
            "Batch: 8000. Validation loss: 0.2379092276096344.\n",
            "Batch: 10000. Validation loss: 0.18341466784477234.\n",
            "Batch: 12000. Validation loss: 0.27230772376060486.\n",
            "Batch: 14000. Validation loss: 0.19694609940052032.\n",
            "Batch: 16000. Validation loss: 0.16090944409370422.\n",
            "Batch: 18000. Validation loss: 0.19287335872650146.\n",
            "Batch: 20000. Validation loss: 0.22472019493579865.\n",
            "Batch: 22000. Validation loss: 0.198243647813797.\n",
            "Batch: 24000. Validation loss: 0.2214808613061905.\n",
            "Batch: 26000. Validation loss: 0.22693906724452972.\n",
            "Batch: 28000. Validation loss: 0.20361603796482086.\n",
            "Batch: 30000. Validation loss: 0.17352131009101868.\n",
            "Batch: 32000. Validation loss: 0.21066558361053467.\n",
            "Batch: 34000. Validation loss: 0.22899678349494934.\n",
            "Batch: 36000. Validation loss: 0.2278604507446289.\n",
            "Batch: 38000. Validation loss: 0.22331096231937408.\n",
            "Batch: 40000. Validation loss: 0.2511116564273834.\n",
            "Batch: 42000. Validation loss: 0.2606673836708069.\n",
            "Batch: 44000. Validation loss: 0.22966918349266052.\n",
            "Batch: 46000. Validation loss: 0.2122492492198944.\n",
            "Batch: 48000. Validation loss: 0.4048255980014801.\n",
            "Batch: 50000. Validation loss: 0.2685982882976532.\n",
            "Batch: 52000. Validation loss: 0.3027723729610443.\n",
            "Batch: 54000. Validation loss: 0.32739341259002686.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/mnist.ckpt\n",
            "Test accuracy: 1.0.\n",
            "---------- batch size = 32 ----------\n",
            "Batch: 0. Validation loss: 242.75157165527344.\n",
            "Batch: 2000. Validation loss: 1.188947081565857.\n",
            "Batch: 4000. Validation loss: 0.47190043330192566.\n",
            "Batch: 6000. Validation loss: 0.25869259238243103.\n",
            "Batch: 8000. Validation loss: 0.21036118268966675.\n",
            "Batch: 10000. Validation loss: 0.20101845264434814.\n",
            "Batch: 12000. Validation loss: 0.18422922492027283.\n",
            "Batch: 14000. Validation loss: 0.17664968967437744.\n",
            "Batch: 16000. Validation loss: 0.2066076397895813.\n",
            "Batch: 18000. Validation loss: 0.16030316054821014.\n",
            "Batch: 20000. Validation loss: 0.16971099376678467.\n",
            "Batch: 22000. Validation loss: 0.20984774827957153.\n",
            "Batch: 24000. Validation loss: 0.13908179104328156.\n",
            "Batch: 26000. Validation loss: 0.18334296345710754.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/mnist.ckpt\n",
            "Test accuracy: 1.0.\n",
            "---------- batch size = 64 ----------\n",
            "Batch: 0. Validation loss: 231.21205139160156.\n",
            "Batch: 2000. Validation loss: 0.7858632802963257.\n",
            "Batch: 4000. Validation loss: 0.43524986505508423.\n",
            "Batch: 6000. Validation loss: 0.24939130246639252.\n",
            "Batch: 8000. Validation loss: 0.19852572679519653.\n",
            "Batch: 10000. Validation loss: 0.18529953062534332.\n",
            "Batch: 12000. Validation loss: 0.18663911521434784.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/mnist.ckpt\n",
            "Test accuracy: 1.0.\n",
            "---------- batch size = 128 ----------\n",
            "Batch: 0. Validation loss: 221.1894989013672.\n",
            "Batch: 2000. Validation loss: 0.684227466583252.\n",
            "Batch: 4000. Validation loss: 0.48093199729919434.\n",
            "Batch: 6000. Validation loss: 0.3159273564815521.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/mnist.ckpt\n",
            "Test accuracy: 0.984375.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-caeab38e5001>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrange_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test accuracy'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3486\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3487\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3563\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3564\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3565\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   3747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3748\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3749\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3750\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3751\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36msanitize_index\u001b[0;34m(data, index, copy)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Length of values does not match length of index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCIndexClass\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Length of values does not match length of index"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3HzLKAalM_y",
        "colab_type": "code",
        "outputId": "4b3499da-9d69-44a1-8e42-a4932def9a7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "df = pd.DataFrame(data = None, index = None)\n",
        "df['batch size'] = range_bs\n",
        "df['test accuracy'] = acc\n",
        "print(df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   batch size  test accuracy\n",
            "0          16       1.000000\n",
            "1          32       1.000000\n",
            "2          64       1.000000\n",
            "3         128       0.984375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXRHYgnBlh6C",
        "colab_type": "text"
      },
      "source": [
        "**Best batch size = 32**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwOY9xb5YBbg",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 2\n",
        "Consider the following function that creates a classification dataset where observations are drawn from multivariate Gaussian distributions:\n",
        "\n",
        "```python\n",
        "def create dataset(means, std , sample_size , seed=None): \n",
        "  random state = np.random.RandomState(seed)\n",
        "\n",
        "  X= np.zeros((sample_size, len(means[0])), dtype=np.float32) \n",
        "  Y = np.zeros((sample_size , len(means)), dtype=np.float32)\n",
        "\n",
        "  cov = np.eye(len(means[0]))∗(std∗∗2)\n",
        "\n",
        "  for i in range(sample_size):\n",
        "    c = random state.randint(len(means))\n",
        "    X[i] = random state.multivariate normal(means[c], cov) \n",
        "    Y[i, c] = 1.\n",
        "\n",
        "  return X, Y\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SljAothiZGfY",
        "colab_type": "text"
      },
      "source": [
        "(a) Using this function, create a dataset with two classes. Place one mean at (−1,1) and another at (1,−1). Let the standard variation be 0.5, the sample size 500, and the seed 0.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dYDI5mFZIeL",
        "colab_type": "text"
      },
      "source": [
        "(b) Use `plt.scatter` to plot the observations in your dataset, coloring the points according to their classes. Tip: use the colors 0 and 1 to represent the classes, and set the colormap (cmap) to `plt.cm.RdBu`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL1zBMckZJ38",
        "colab_type": "text"
      },
      "source": [
        "(c) Train a multilayer perceptron using your dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmGnInkvKfAS",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}